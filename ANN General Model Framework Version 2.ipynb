{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytLctrRns5dY"
   },
   "source": [
    "# 人工神經網路的簡單通用框架\n",
    "\n",
    "進一步再分離加權與啟動函式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1769694986728,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "AWqqj9n7T3LW"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFj_tLs6Txbi"
   },
   "source": [
    "##  二元交叉熵損失函數與梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1769694986746,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "aes0kGIbTwp7"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Sigmoid 函數，常用在二元分類，將輸入值壓到 0~1 之間\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def binary_cross_entropy(f, y, epsilon=1e-8):\n",
    "    # 二元交叉熵損失函數\n",
    "    # f: 模型預測值 (機率)\n",
    "    # y: 真實標籤 (0 或 1)\n",
    "    # epsilon: 避免 log(0) 用的小常數\n",
    "\n",
    "    # 原本可能是想逐筆算 loss（被註解掉）\n",
    "    # np.sum(y*np.log(f+epsilon)+ (1-y)*np.log(1-f+epsilon), axis=1)\n",
    "\n",
    "    m = len(y)  # 資料筆數\n",
    "    # 計算整體平均 loss\n",
    "    return - (1./m) * np.sum(\n",
    "        np.multiply(y, np.log(f + epsilon)) +\n",
    "        np.multiply((1 - y), np.log(1 - f + epsilon))\n",
    "    )\n",
    "\n",
    "def binary_cross_entropy_grad(out, y, sigmoid_out=True, epsilon=1e-8):\n",
    "    # 計算 binary cross entropy 對輸出值的梯度\n",
    "    # out: 模型輸出\n",
    "    # y: 真實標籤\n",
    "    # sigmoid_out: 表示 out 是否已經過 sigmoid\n",
    "\n",
    "    if sigmoid_out:\n",
    "        f = out  # 如果已經是 sigmoid 輸出，直接使用\n",
    "        grad = ((f - y) / (f * (1 - f) + epsilon)) / (len(y))\n",
    "        # f = np.clip(f, eps, 1 - eps) # 建議寫法\n",
    "        # grad = ((f - y) / (f * (1 - f))) / (len(y))\n",
    "    else:\n",
    "        f = sigmoid(out)  # 如果 out 是 z，先做 sigmoid\n",
    "        grad = (f - y) / (len(y))\n",
    "\n",
    "def binary_cross_entropy_loss_grad(out, y, sigmoid_out=True, epsilon=1e-8):\n",
    "    # 同時計算 loss 與 gradient\n",
    "    # 常用在訓練時一次回傳兩個結果\n",
    "\n",
    "    if sigmoid_out:\n",
    "        f = out  # 已經是 sigmoid 結果\n",
    "        grad = ((f - y) / (f * (1 - f) + epsilon)) / (len(y))\n",
    "        # f = np.clip(f, eps, 1 - eps) # 建議寫法\n",
    "        # grad = ((f - y) / (f * (1 - f))) / (len(y))\n",
    "    else:\n",
    "        f = sigmoid(out)  # out 是尚未經過 sigmoid 的值\n",
    "        grad = (f - y) / (len(y))\n",
    "\n",
    "    # 計算 binary cross entropy loss\n",
    "    loss = binary_cross_entropy(f, y, epsilon)\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55t1c6IQUCyV"
   },
   "source": [
    "## softmax交叉熵損失函數與梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1769694986751,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "hsIf2ikVUBzG"
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    # 對輸入的 Z 做 softmax，先減掉最大值避免數值爆掉\n",
    "    A = np.exp(Z - np.max(Z, axis=-1, keepdims=True))\n",
    "    # 將指數結果做正規化，讓每一列加起來等於 1\n",
    "    return A / np.sum(A, axis=-1, keepdims=True)\n",
    "\n",
    "def softmax_cross_entropy(Z, y, onehot=False):\n",
    "    # 樣本的總數（有幾筆資料）\n",
    "    m = len(Z)\n",
    "\n",
    "    # 先把模型輸出 Z 丟進 softmax，算出每一類的機率\n",
    "    F = softmax(Z)\n",
    "\n",
    "    if onehot:\n",
    "        # 如果 y 是 one-hot 編碼\n",
    "        # 直接用 cross entropy 的公式算 loss\n",
    "        loss = -np.sum(y * np.log(F)) / m\n",
    "    else:\n",
    "        # 如果 y 是類別的 index（例如 0、1、2）\n",
    "        # 把 y 攤平成一維（但這行實際上不會影響後面）\n",
    "        y.flatten()\n",
    "\n",
    "        # 取出每筆資料「正確類別」對應的機率，再取 log\n",
    "        log_Fy = -np.log(F[range(m), y])\n",
    "\n",
    "        # 把所有樣本的 loss 加起來，再取平均\n",
    "        loss = np.sum(log_Fy) / m\n",
    "\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_grad(Z, Y, onehot=False, softmax_out=False):\n",
    "    # 判斷輸入的 Z 是不是已經是 softmax 的輸出\n",
    "    if softmax_out:\n",
    "        F = Z\n",
    "    else:\n",
    "        # 如果不是，就先做一次 softmax\n",
    "        F = softmax(Z)\n",
    "\n",
    "    if onehot:\n",
    "        # 如果 Y 是 one-hot 編碼，直接用 F - Y\n",
    "        grad = (F - Y) / len(Z)\n",
    "    else:\n",
    "        # 如果 Y 是類別索引（例如 [0, 2, 1]）\n",
    "        m = len(Y)          # 樣本數\n",
    "        grad = F.copy()       # 複製一份避免動到原本的 F\n",
    "        # 對正確類別的位置減 1\n",
    "        grad[np.arange(m), Y] -= 1\n",
    "        # 對 batch size 做平均\n",
    "        grad /= m\n",
    "\n",
    "        # 以下是另一種寫法，這裡先註解起來\n",
    "        # I_i = np.zeros_like(Z)\n",
    "        # I_i[np.arange(len(Z)),Y] = 1\n",
    "        # return (F - I_i) /len(Z)  #Z.shape[0]\n",
    "\n",
    "    # 回傳對 Z 的梯度\n",
    "    return grad\n",
    "\n",
    "# def cross_entropy_grad_loss(F, y, softmax_out=False, onehot=False):\n",
    "#     # 判斷輸入的 F 是否已經經過 Softmax 處理\n",
    "#     if softmax_out:\n",
    "#         # 如果已經是機率分布了，直接丟進去算 Loss\n",
    "#         loss = softmax_cross_entropy(F, y, onehot)\n",
    "#         # 註解掉的這行可能是想區分一般的 Cross Entropy 與 Softmax 版\n",
    "#         # loss = cross_entropy_loss(F, y, onehot)\n",
    "#     else:\n",
    "#         # 如果 F 還是 Logits（原始輸出），則執行含 Softmax 的 Cross Entropy 計算\n",
    "#         loss = softmax_cross_entropy(F, y, onehot)\n",
    "\n",
    "#     # 計算梯度（Grad），同樣會根據 softmax_out 決定內部是否要補做 Softmax\n",
    "#     grad = cross_entropy_grad(F, y, onehot, softmax_out)\n",
    "\n",
    "#     # 同時回傳損失值與梯度，方便後向傳播使用\n",
    "#     return loss, grad\n",
    "\n",
    "def cross_entropy_grad_loss(Z, y, softmax_out=False, onehot=False):\n",
    "    \"\"\"\n",
    "    優化後的函式：共用 Softmax 運算結果，同時回傳 Loss 與 Gradient。\n",
    "    \"\"\"\n",
    "    # 1. 決定機率分布 F (Softmax 的結果)\n",
    "    if softmax_out:\n",
    "        # 如果已經是 Softmax 輸出，直接共用\n",
    "        F = Z\n",
    "    else:\n",
    "        # 如果是 Logits，算一次 Softmax 就好，後面大家都用這份 F\n",
    "        F = softmax(Z)\n",
    "\n",
    "    # 2. 計算 Loss\n",
    "    # 注意：這裡我們傳入已經算好的 F，並告訴後面的函式 softmax_out=True\n",
    "    # (假設你原有的 softmax_cross_entropy 有支援這個判斷，或是我們直接在這裡算)\n",
    "    m = len(y)\n",
    "    if onehot:\n",
    "        loss = -np.sum(y * np.log(F + 1e-12)) / m\n",
    "    else:\n",
    "        # 這裡示範直接計算，減少函式呼叫的開銷\n",
    "        loss = -np.sum(np.log(F[np.arange(m), y] + 1e-12)) / m\n",
    "\n",
    "    # 3. 計算梯度 (Gradient)\n",
    "    # 直接利用剛剛算好的 F 進行計算，效能最優\n",
    "    grad = F.copy()\n",
    "    if onehot:\n",
    "        grad = (grad - y) / m\n",
    "    else:\n",
    "        grad[np.arange(m), y] -= 1\n",
    "        grad /= m\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgfz1ayhVBoT"
   },
   "source": [
    "## 平均平方誤差損失函數與梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1769694986766,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "AmbYrSyr0SWF"
   },
   "outputs": [],
   "source": [
    "def mse_loss(F, Y, divid_2 = False):\n",
    "    m = F.shape[0]\n",
    "    loss =  np.sum((F - Y) ** 2)/m\n",
    "    # loss = np.mean((F - Y)**2, axis=0).sum()\n",
    "    if divid_2:\n",
    "        loss /= 2\n",
    "    return loss\n",
    "\n",
    "def mse_loss_grad(f, y):\n",
    "    m = len(f)\n",
    "    # 計算資料筆數（樣本數）\n",
    "\n",
    "    loss = (1./m) * np.sum((f - y) ** 2)\n",
    "    # 計算 MSE（均方誤差），用來衡量預測值跟實際值的差距\n",
    "\n",
    "    grad = (2./m) * (f - y)\n",
    "    # 計算 MSE 對預測值 f 的梯度，常用在梯度下降\n",
    "\n",
    "    return loss, grad\n",
    "    # 回傳 loss 值以及對應的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xcEmOcfVQVd"
   },
   "source": [
    "## 其他函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1769694986826,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "rxjvEpuXVSWU"
   },
   "outputs": [],
   "source": [
    "def dRelu(x):\n",
    "    # ReLU 的微分：x > 0 回傳 1，否則回傳 0\n",
    "    return 1 * (x > 0)\n",
    "\n",
    "def max_abs(s):\n",
    "    # 計算一組矩陣或向量中，所有元素的最大絕對值\n",
    "    max_value = 0\n",
    "    for x in s:\n",
    "        max_value_ = np.max(np.abs(x))  # 取單一矩陣的最大絕對值\n",
    "        if(max_value_ > max_value):\n",
    "            max_value = max_value_\n",
    "    return max_value\n",
    "\n",
    "# 產生一個非線性可分（Non-linearly separable）的「旋渦狀」資料集\n",
    "def gen_spiral_dataset(N=100, D=2, K=3):\n",
    "    # 初始化特徵矩陣 X，形狀為 (樣本總數, 維度)，這裡預設產生 300 筆 2 維資料\n",
    "    X = np.zeros((N*K, D))  # 建立資料矩陣，每一列代表一筆資料\n",
    "\n",
    "    # 初始化標籤陣列 y，記錄每筆資料屬於哪一個類別（0, 1, 2）\n",
    "    y = np.zeros(N*K, dtype='uint8')  # 建立標籤陣列，用來存每筆資料的類別\n",
    "\n",
    "    for j in range(K):  # 依序產生每一個類別 (Class) 的資料\n",
    "        # 計算目前類別在矩陣中對應的索引範圍 (Index range)\n",
    "        ix = range(N*j, N*(j+1))\n",
    "\n",
    "        # 產生半徑 r，從 0 到 1 均勻分布，代表點從原點往外擴散\n",
    "        r = np.linspace(0.0, 1, N)  # 半徑，從中心往外均勻增加\n",
    "\n",
    "        # 產生角度 t，隨類別不同有不同的起始角度，並加入高斯雜訊 (Noise) 讓資料不會太死板\n",
    "        t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.2  # 角度，加上一點雜訊讓資料比較分散\n",
    "\n",
    "        # 利用極座標轉直角座標 (r*sin(t), r*cos(t))，並用 np.c_ 進行欄位合併\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]  # 轉成平面座標，形成螺旋狀\n",
    "\n",
    "        # 填入對應的類別編號\n",
    "        y[ix] = j  # 設定對應的類別標籤\n",
    "\n",
    "    return X, y  # 回傳產生的特徵矩陣 X 與目標標籤 y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY26sqsnVX7X"
   },
   "source": [
    "## 神經網路層子類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1769694986831,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "kFKfDdhbtJXv"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.params = None\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):       \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, x, grad):        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reg_grad(self, reg):\n",
    "        pass\n",
    "\n",
    "    def reg_loss(self, reg):\n",
    "        return 0.  \n",
    "    \n",
    "#---------- 加權總和計算 (全連接層) ------------    \n",
    "class Dense(Layer): \n",
    "    # 計算公式：Z = XW + b\n",
    "    def __init__(self, input_dim, out_dim, init_method=('random', 0.01)):  \n",
    "        super().__init__()\n",
    "        random_method_name, random_value = init_method      \n",
    "        \n",
    "        # 根據不同的初始化方法設定權重 (W) 與偏置項 (b)\n",
    "        if random_method_name == \"random\":\n",
    "            # 一般隨機初始化\n",
    "            self.W = np.random.randn(input_dim, out_dim) * random_value\n",
    "            self.b = np.random.randn(1, out_dim) * random_value  \n",
    "        elif random_method_name == \"he\":\n",
    "            # He 初始化：適用於 ReLU 激活函數，保持變異數一致\n",
    "            self.W = np.random.randn(input_dim, out_dim) * np.sqrt(2 / input_dim)\n",
    "            self.b = np.zeros((1, out_dim))\n",
    "        elif random_method_name == \"xavier\":\n",
    "            # Xavier 初始化 (Glorot)：適用於 Sigmoid/Tanh，平衡層與層之間的訊號\n",
    "            self.W = np.random.randn(input_dim, out_dim) * np.sqrt(1 / input_dim)\n",
    "            self.b = np.random.randn(1, out_dim) * random_value  \n",
    "        elif random_method_name == \"zeros\":\n",
    "            # 全零初始化（通常不建議對 W 使用，會導致神經元對稱）\n",
    "            self.W = np.zeros((input_dim, out_dim))\n",
    "            self.b = np.zeros((1, out_dim))   \n",
    "        else:            \n",
    "            self.W = np.random.randn(input_dim, out_dim) * random_value\n",
    "            self.b = np.zeros((1, out_dim))  \n",
    "            \n",
    "        self.params = [self.W, self.b]\n",
    "        # 儲存對應參數的梯度\n",
    "        self.grads = [np.zeros_like(self.W), np.zeros_like(self.b)]\n",
    "        \n",
    "    def forward(self, x): \n",
    "        self.x = x        \n",
    "        # 攤平 (Flatten)：將多維度輸入（如影像卷積後的結果）轉換為 2D 矩陣 (Batch_size, Features)\n",
    "        x1 = x.reshape(x.shape[0], np.prod(x.shape[1:]))       \n",
    "        # 矩陣相乘並加上偏置項\n",
    "        Z = np.matmul(x1, self.W) + self.b        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        # 反向傳播 (Backpropagation)\n",
    "        x = self.x\n",
    "        # 同樣進行攤平操作以計算梯度\n",
    "        x1 = x.reshape(x.shape[0], np.prod(x.shape[1:]))\n",
    "        \n",
    "        # 計算參數梯度：dW = X.T * dZ, db = sum(dZ)\n",
    "        dW = np.dot(x1.T, dZ)\n",
    "        db = np.sum(dZ, axis=0, keepdims=True)          \n",
    "        \n",
    "        # 計算對輸入 x 的梯度 (dx)，用於傳回前一層\n",
    "        dx = np.dot(dZ, np.transpose(self.W)) \n",
    "        # 將 dx 還原成與原始輸入相同的形狀 (Reverse Flatten)\n",
    "        dx = dx.reshape(x.shape)    \n",
    "        \n",
    "        # 梯度累加，以便在 Optimizer 更新前儲存多個 Batch 的結果\n",
    "        self.grads[0] += dW\n",
    "        self.grads[1] += db\n",
    "       \n",
    "        return dx\n",
    "    \n",
    "    #-------- 添加正規化項 (L2 Regularization) 的梯度 -----\n",
    "    def reg_grad(self, reg):\n",
    "        # L2 正規化梯度：2 * reg * W\n",
    "        self.grads[0] += 2 * reg * self.W\n",
    "        \n",
    "    def reg_loss(self, reg):\n",
    "        # L2 正規化損失值：reg * sum(W^2)\n",
    "        return reg * np.sum(self.W**2)\n",
    "    \n",
    "    def reg_loss_grad(self, reg):\n",
    "        # 同時回傳損失並累加梯度\n",
    "        self.grads[0] += 2 * reg * self.W\n",
    "        return reg * np.sum(self.W**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bxrp74ZwtVUL"
   },
   "source": [
    "## 測試：人工神經網路的向前函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1769694986842,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "_HfaLrjgtU1t",
    "outputId": "8ca453e8-44a4-449b-812f-e25b0c5573d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n",
      "[[-0.03953509 -0.00214997  0.00743433 -0.16926214 -0.05162853  0.06734225\n",
      "  -0.00221485 -0.11710758 -0.07046456  0.02609659]\n",
      " [ 0.00848392  0.08259757 -0.09858177  0.0374092  -0.08303008  0.04151241\n",
      "  -0.01407859 -0.02415486  0.04236149  0.0648261 ]\n",
      " [-0.13877363 -0.04122276 -0.00984716 -0.03461381  0.11513754  0.1043094\n",
      "   0.00170353 -0.00449278 -0.0057236  -0.01403174]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 設定隨機種子，確保每次產生的隨機數結果都一樣，方便除錯與重現結果\n",
    "np.random.seed(1)\n",
    "\n",
    "# 產生一個隨機矩陣\n",
    "# 形狀為 (3, 3, 4, 4)，代表 3 筆資料，每筆資料有 3 個通道，每個通道是 4x4 的圖像\n",
    "x = np.random.randn(3, 3, 4, 4) # 3 個樣本，每個樣本有 48 維特徵\n",
    "\n",
    "# 建立一個全連接層 (Dense Layer)\n",
    "# 輸入維度為 48，輸出維度為 10，啟用函數設定為 'none'\n",
    "dense = Dense(3 * 4 * 4, 10, ('no', 0.01))\n",
    "\n",
    "# 將輸入資料丟進 Dense 層做前向傳播\n",
    "o = dense.forward(x)\n",
    "\n",
    "# 輸出結果的形狀\n",
    "print(o.shape)\n",
    "\n",
    "# 輸出實際計算結果\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1769694986850,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "TRxVojINvxPC"
   },
   "outputs": [],
   "source": [
    "def numerical_gradient_from_df(f, p, df, h=1e-5):\n",
    "  # 建立一個與 p 形狀相同、內容全為 0 的陣列，用來存每個參數的梯度\n",
    "  grad = np.zeros_like(p)\n",
    "\n",
    "  # 使用 nditer 逐一走訪 p 中的每一個元素（支援多維陣列）\n",
    "  it = np.nditer(p, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "  # 只要 iterator 還沒跑完就持續計算\n",
    "  while not it.finished:\n",
    "    # 取得目前走訪到的索引位置\n",
    "    idx = it.multi_index\n",
    "\n",
    "    # 先把原本的參數值存起來\n",
    "    oldval = p[idx]\n",
    "\n",
    "    # 將該參數往正方向微調一點\n",
    "    p[idx] = oldval + h\n",
    "    pos = f()       # 在參數被改動後重新呼叫 f()，取得正方向的輸出結果\n",
    "\n",
    "    # 將該參數往負方向微調一點\n",
    "    p[idx] = oldval - h\n",
    "    neg = f()       # 在參數被改動後重新呼叫 f()，取得負方向的輸出結果\n",
    "\n",
    "    # 將參數值還原成原本的狀態，避免影響下一次計算\n",
    "    p[idx] = oldval\n",
    "\n",
    "    # 使用中央差分法計算梯度，並與上游傳下來的 df 做加權\n",
    "    grad[idx] = np.sum((pos - neg) * df) / (2 * h)\n",
    "\n",
    "    # 另一種寫法（使用內積），目前被註解掉\n",
    "    # grad[idx] = np.dot((pos - neg), df) / (2 * h)\n",
    "\n",
    "    # 移動到下一個參數位置\n",
    "    it.iternext()\n",
    "\n",
    "  # 回傳整個參數 p 的數值梯度\n",
    "  return grad\n",
    "\n",
    "# 通用數值梯度函式\n",
    "def numerical_gradient(f, params, eps=1e-6):\n",
    "    numerical_grads = []  # 用來存每一個參數對應的梯度\n",
    "\n",
    "    for x in params:\n",
    "        # x 可能是多維陣列，這裡會針對 x 裡面的每一個元素計算數值梯度\n",
    "        grad = np.zeros(x.shape)  # 建立一個跟 x 形狀一樣的陣列來存梯度\n",
    "\n",
    "        # 使用 nditer 逐一走訪 x 裡面的每個元素\n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index   # 目前元素的索引位置\n",
    "\n",
    "            old_value = x[idx]     # 先把原本的值存起來\n",
    "\n",
    "            x[idx] = old_value + eps  # 對目前的值加上一個很小的 eps\n",
    "            fx = f()                  # 計算 f(x + eps) 的結果\n",
    "\n",
    "            x[idx] = old_value - eps  # 對目前的值減去一個很小的 eps\n",
    "            fx_ = f()                 # 計算 f(x - eps) 的結果\n",
    "\n",
    "            # 使用中央差分法計算數值梯度\n",
    "            grad[idx] = (fx - fx_) / (2 * eps)\n",
    "\n",
    "            x[idx] = old_value        # 記得把參數值還原，避免影響後續計算\n",
    "            it.iternext()             # 移動到下一個元素\n",
    "\n",
    "        numerical_grads.append(grad)  # 將目前參數的梯度存起來\n",
    "\n",
    "    return numerical_grads            # 回傳所有參數的梯度\n",
    "\n",
    "# def f():\n",
    "#     return compute_loss_reg(forward_propagation, softmax_cross_entropy_reg, X, y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2878696455368005e-09\n",
      "2.371454232453651e-11\n",
      "[[ 1.77463167  0.11663492  1.87794917  0.27986781  1.27243915 -2.44375556\n",
      "  -2.1266117   0.99629747 -0.73720237 -0.68570287]\n",
      " [-0.69807196  0.22547472 -0.93721649  0.3286185  -1.0421723   0.66487528\n",
      "   1.33111205  0.25677848 -0.58451408  0.71015412]\n",
      " [ 0.12251147 -0.4041516   0.57764614  0.89962639 -0.35195022  0.77829011\n",
      "  -0.01618803 -0.62209694 -1.28543176 -0.37554316]]\n",
      "[[ 1.77463167  0.11663492  1.87794917  0.27986781  1.27243915 -2.44375556\n",
      "  -2.1266117   0.99629747 -0.73720237 -0.68570287]\n",
      " [-0.69807196  0.22547472 -0.93721649  0.3286185  -1.0421723   0.66487528\n",
      "   1.33111205  0.25677848 -0.58451408  0.71015412]\n",
      " [ 0.12251147 -0.4041516   0.57764614  0.89962639 -0.35195022  0.77829011\n",
      "  -0.01618803 -0.62209694 -1.28543176 -0.37554316]]\n"
     ]
    }
   ],
   "source": [
    "do = np.random.randn(3, 10)\n",
    "dx = dense.backward(do)\n",
    "dx_num = numerical_gradient_from_df(lambda :dense.forward(x),x,do)\n",
    "\n",
    "diff_error = lambda x, y: np.max(np.abs(x - y)/(np.maximum(1e-8, np.abs(x) + np.abs(y) )) )\n",
    "print(diff_error(dx,dx_num))\n",
    "\n",
    "dW_num = numerical_gradient_from_df(lambda :dense.forward(x),dense.params[0],do)\n",
    "print(diff_error(dense.grads[0],dW_num))\n",
    "print(dense.grads[0][:3])\n",
    "print(dW_num[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0148861341442835e-07\n",
      "[[ 0.47568681 -0.06324119 -0.29294422 -0.76304343 -0.09660146  0.62794569\n",
      "   1.16087896  0.06261028 -0.6611078  -0.02940735]\n",
      " [-0.10777785 -1.47174583  0.63258553  1.22381944 -0.35702633  0.4409597\n",
      "  -2.42444873 -0.28804741 -1.33377026  0.66775208]]\n",
      "[array([ 0.47568681, -0.06324119, -0.29294422, -0.76304343, -0.09660146,\n",
      "        0.62794569,  1.16087896,  0.06261028, -0.6611078 , -0.02940735]), array([-0.10777785, -1.47174583,  0.63258553,  1.22381944, -0.35702633,\n",
      "        0.4409597 , -2.42444873, -0.28804741, -1.33377026,  0.66775208])]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3,3,4, 4)\n",
    "y = np.random.randn(3,10) \n",
    "\n",
    "dense = Dense(3*4*4,10,('no',0.01))\n",
    "\n",
    "f = dense.forward(x)\n",
    "loss,do = mse_loss_grad(f,y)\n",
    "dx = dense.backward(do)\n",
    "def loss_f():\n",
    "    f = dense.forward(x)\n",
    "    loss= mse_loss(f,y)\n",
    "    return loss\n",
    "    \n",
    "dW_num = numerical_gradient(loss_f,dense.params[0],1e-6)\n",
    "print(diff_error(dense.grads[0],dW_num))\n",
    "print(dense.grads[0][:2])\n",
    "print(dW_num[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 啟動函數層的類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        self.x = x  \n",
    "        # ReLU 函數：輸入大於 0 則原樣輸出，小於等於 0 則輸出 0\n",
    "        return np.maximum(0, x)\n",
    "    def backward(self, grad_output):\n",
    "        # ReLU 的導數：x > 0 時導數為 1，否則為 0\n",
    "        x = self.x\n",
    "        relu_grad = x > 0\n",
    "        # 根據連鎖律，將後方傳來的梯度 (grad_output) 乘上本層梯度\n",
    "        return grad_output * relu_grad \n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        self.x = x  \n",
    "        # Sigmoid 函數：將輸出壓縮到 (0, 1) 之間\n",
    "        return 1.0 / (1.0 + np.exp(-x))     \n",
    "    def backward(self, grad_output): \n",
    "        x = self.x  \n",
    "        # 使用 forward 計算出的結果來計算 Sigmoid 的導數：a * (1 - a)\n",
    "        a = 1.0 / (1.0 + np.exp(-x))         \n",
    "        return grad_output * a * (1 - a) \n",
    "    \n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        self.x = x  \n",
    "        # Tanh 函數：將輸出壓縮到 (-1, 1) 之間，基底以 0 為中心 (Zero-centered)\n",
    "        self.a = np.tanh(x)  \n",
    "        return self.a    \n",
    "    def backward(self, grad_output):           \n",
    "        # Tanh 的導數公式：1 - tanh(x)^2\n",
    "        d = (1 - np.square(self.a))           \n",
    "        return grad_output * d\n",
    "    \n",
    "class Leaky_relu(Layer):\n",
    "    def __init__(self, leaky_slope):\n",
    "        super().__init__()\n",
    "        self.leaky_slope = leaky_slope        \n",
    "    def forward(self, x):\n",
    "        self.x = x  \n",
    "        # Leaky ReLU：解決 ReLU 在負數區域「神經元壞死」(Dead ReLU) 的問題\n",
    "        return np.maximum(self.leaky_slope * x, x)            \n",
    "    def backward(self, grad_output): \n",
    "        x = self.x    \n",
    "        # 初始化與 x 形狀相同的梯度矩陣\n",
    "        d = np.zeros_like(x)\n",
    "        # 設定負數區域的斜率 (Leaky Slope) 與正數區域的斜率 (1)\n",
    "        d[x <= 0] = self.leaky_slope\n",
    "        d[x > 0] = 1       \n",
    "        return grad_output * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2756345281587516e-12\n",
      "7.43892997215858e-12\n",
      "3.830303399950655e-11\n",
      "3.282573028416693e-11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3,3,4, 4)\n",
    "do = np.random.randn(3,3,4, 4)\n",
    "\n",
    "relu = Relu()\n",
    "relu.forward(x)\n",
    "dx = relu.backward(do)\n",
    "dx_num = numerical_gradient_from_df(lambda :relu.forward(x),x,do)\n",
    "print(diff_error(dx,dx_num))\n",
    "\n",
    "leaky_relu = Leaky_relu(0.1)\n",
    "leaky_relu.forward(x)\n",
    "dx = leaky_relu.backward(do)\n",
    "dx_num = numerical_gradient_from_df(lambda :leaky_relu.forward(x),x,do)\n",
    "print(diff_error(dx,dx_num))\n",
    "\n",
    "tanh = Tanh()\n",
    "tanh.forward(x)\n",
    "dx = tanh.backward(do)\n",
    "dx_num = numerical_gradient_from_df(lambda :tanh.forward(x),x,do)\n",
    "print(diff_error(dx,dx_num))\n",
    "\n",
    "sigmoid = Sigmoid()\n",
    "sigmoid.forward(x)\n",
    "dx = sigmoid.backward(do)\n",
    "dx_num = numerical_gradient_from_df(lambda :sigmoid.forward(x),x,do)\n",
    "print(diff_error(dx,dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sa-jWTJ1Vdfj"
   },
   "source": [
    "## 人工神經網路類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1769694986896,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "oEJMZ0snwTfJ"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:  \n",
    "    def __init__(self):\n",
    "        # 儲存網路中所有的層 (如 Dense, Relu 等)\n",
    "        self._layers = []\n",
    "        # 儲存所有可訓練參數 (W, b) 及其對應梯度的引用 (Reference)\n",
    "        self._params = []\n",
    " \n",
    "    def add_layer(self, layer):      \n",
    "        self._layers.append(layer)\n",
    "        # 如果該層具有參數（例如 Dense 層），則將其參數與梯度存入總表\n",
    "        if layer.params: \n",
    "            for i, _ in enumerate(layer.params):                         \n",
    "                # 這裡存入的是 list [參數, 梯度]，方便後續 Optimizer 存取\n",
    "                self._params.append([layer.params[i], layer.grads[i]])            \n",
    "    \n",
    "    def forward(self, X): \n",
    "        # 依序執行每一層的前向傳播，將上一層的輸出作為下一層的輸入\n",
    "        for layer in self._layers:\n",
    "            X = layer.forward(X) \n",
    "        return X   \n",
    "\n",
    "    def __call__(self, X):\n",
    "        # 讓物件可以像函式一樣被調用，例如 model(X)\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        輸入資料 X，預測其類別標籤 (Class Label)\n",
    "        \"\"\"\n",
    "        p = self.forward(X)\n",
    "        # 處理單一樣本的情況\n",
    "        if p.ndim == 1:\n",
    "            return np.argmax(p)        \n",
    "        # 處理多樣本 (Batch)，回傳機率最大的索引值\n",
    "        return np.argmax(p, axis=1)\n",
    "  \n",
    "    def backward(self, loss_grad, reg=0.):\n",
    "        # 從最後一層開始往回計算梯度 (反向疊代)\n",
    "        for i in reversed(range(len(self._layers))):\n",
    "            layer = self._layers[i] \n",
    "            # 計算該層的梯度，並更新傳回前一層的 loss_grad\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            # 加上正規化項的梯度 (如 L2 Regularization)\n",
    "            layer.reg_grad(reg) \n",
    "        return loss_grad\n",
    "    \n",
    "    def backpropagation(self, X, y, loss_function, reg=0):\n",
    "        \"\"\"\n",
    "        執行完整的反向傳播流程，計算所有參數的梯度\n",
    "        \"\"\"        \n",
    "        # 1. 前向傳播取得預測結果\n",
    "        f = self.forward(X)          \n",
    "        # 2. 透過損失函數計算 Loss 值與輸出層的初始梯度\n",
    "        loss, loss_grad = loss_function(f, y)         \n",
    "      \n",
    "        # 3. 清空舊的梯度，避免累加錯誤\n",
    "        self.zero_grad()\n",
    "        # 4. 執行反向傳播計算各層梯度\n",
    "        self.backward(loss_grad)  \n",
    "        # 5. 計算正規化損失\n",
    "        reg_loss = self.reg_loss(reg)       \n",
    "        return loss + reg_loss\n",
    "    \n",
    "    def reg_loss(self, reg):\n",
    "        # 累加網路中所有層的正規化損失\n",
    "        total_reg_loss = 0\n",
    "        for i in range(len(self._layers)):\n",
    "            total_reg_loss += self._layers[i].reg_loss(reg)\n",
    "        return total_reg_loss\n",
    "    \n",
    "    def parameters(self): \n",
    "        # 回傳模型所有的參數與梯度對\n",
    "        return self._params\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        # 將所有參數的梯度歸零，這是每次迭代前的必要步驟\n",
    "        for i, _ in enumerate(self._params):           \n",
    "            # 使用切片 [:] 確保是在原位 (In-place) 修改數值\n",
    "            self._params[i][1][:] = 0 \n",
    "            \n",
    "    def get_parameters(self):\n",
    "        return self._params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1769694986904,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "SeRWM_JFwXRf",
    "outputId": "1453cc39-e044-41b6-b96c-d07aeaa7769a"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m nn = NeuralNetwork()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 新增第一層全連接層：輸入 2 維，輸出 100 維，使用 ReLU 當啟動函數\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m nn.add_layer(\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 新增第二層全連接層：輸入 100 維，輸出 3 維，使用 softmax 做分類\u001b[39;00m\n\u001b[32m      8\u001b[39m nn.add_layer(Dense(\u001b[32m100\u001b[39m, \u001b[32m3\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mDense.__init__\u001b[39m\u001b[34m(self, input_dim, out_dim, init_method)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim, out_dim, init_method=(\u001b[33m'\u001b[39m\u001b[33mrandom\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.01\u001b[39m)):  \n\u001b[32m     22\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     random_method_name, random_value = init_method      \n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# 根據不同的初始化方法設定權重 (W) 與偏置項 (b)\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m random_method_name == \u001b[33m\"\u001b[39m\u001b[33mrandom\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     27\u001b[39m         \u001b[38;5;66;03m# 一般隨機初始化\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 建立一個神經網路物件\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# 新增第一層全連接層：輸入 2 維，輸出 100 維，使用 ReLU 當啟動函數\n",
    "nn.add_layer(Dense(2, 100, 'relu'))\n",
    "\n",
    "# 新增第二層全連接層：輸入 100 維，輸出 3 維，使用 softmax 做分類\n",
    "nn.add_layer(Dense(100, 3, 'softmax'))\n",
    "\n",
    "# 隨機產生一組輸入資料（2 筆資料，每筆 2 維）\n",
    "X_temp = np.random.randn(2,2)\n",
    "\n",
    "# 隨機產生對應的標籤（類別數為 3）\n",
    "y_temp = np.random.randint(3, size=2)\n",
    "\n",
    "# 將資料丟進神經網路做前向傳播\n",
    "F = nn.forward(X_temp)\n",
    "\n",
    "# 計算 softmax cross entropy 的 loss\n",
    "loss = softmax_cross_entropy(F,y_temp)\n",
    "\n",
    "# 計算 cross entropy 對輸出的梯度\n",
    "loss_grad =  cross_entropy_grad(F,y_temp)\n",
    "\n",
    "# 印出 loss 以及梯度的平均值，方便快速檢查結果\n",
    "print(loss, np.mean(loss_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1769694987027,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "XbiaVNOvw_3A",
    "outputId": "1d10137a-c761-48fe-8d79-8e070914db09"
   },
   "outputs": [],
   "source": [
    "# 1. 執行後向傳播 (Backward Pass)\n",
    "# 根據損失函數對輸出的梯度 (loss_grad) 來計算模型中所有參數的梯度\n",
    "nn.backward(loss_grad)\n",
    "\n",
    "# 從網路物件中抓出剛剛算好的所有參數梯度 (解析解)\n",
    "grads = nn.grads()\n",
    "\n",
    "# 2. 定義一個用來做數值微分的閉包 (Closure)\n",
    "def loss_fun():\n",
    "    # 這裡會用到暫存的資料 X_temp\n",
    "    F = nn.forward(X_temp)\n",
    "    # 回傳當下的損失值 (Loss)\n",
    "    return softmax_cross_entropy(F, y_temp)\n",
    "\n",
    "# 3. 取得模型內所有的參數 (Parameters)，包含各層的 W 和 b\n",
    "params = nn.parameters()\n",
    "\n",
    "# 利用數值方法 (如中心差分) 算出每個參數的數值梯度 (Numerical Gradient)\n",
    "# 1e-6 是微小的偏移量 epsilon\n",
    "numerical_grads = numerical_gradient(loss_fun, params, 1e-6)\n",
    "\n",
    "# 4. 檢查兩者的形狀是否一致 (Shape Check)\n",
    "for i in range(len(params)):\n",
    "    # 確保數值梯度的矩陣形狀跟解析梯度完全一樣\n",
    "    print(numerical_grads[i].shape, grads[i].shape)\n",
    "\n",
    "# 5. 定義誤差計算函式\n",
    "def diff_error(x, y):\n",
    "    # 計算單一參數矩陣的最大絕對誤差\n",
    "    return np.max(np.abs(x - y))\n",
    "\n",
    "def diff_errors(xs, ys):\n",
    "    errors = []\n",
    "    # 遍歷所有層的參數梯度\n",
    "    for i in range(len(xs)):\n",
    "        errors.append(diff_error(xs[i], ys[i]))\n",
    "    # 回傳所有參數中「最大」的那一個誤差值\n",
    "    return np.max(errors)\n",
    "\n",
    "# 6. 最後輸出總體誤差\n",
    "# 如果這個值小於 1e-7，通常代表你的 Backprop 寫得非常完美！\n",
    "diff_errors(numerical_grads, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1769694987027,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "HWRv2sT11vAS"
   },
   "outputs": [],
   "source": [
    "def train(nn, X, y, loss_function, epochs=10000, learning_rate=1e-0, reg=1e-3, print_n=10):\n",
    "    # 開始執行指定次數的訓練循環 (Epochs)\n",
    "    for epoch in range(epochs):\n",
    "        # 1. 前向傳播 (Forward Pass)：將資料丟進網路，得到預測結果 f (Logits)\n",
    "        f = nn.forward(X)\n",
    "\n",
    "        # 2. 計算損失與梯度：使用傳入的 loss_function 算出 Loss 以及對應的梯度 loss_grad\n",
    "        loss, loss_grad = loss_function(f, y)\n",
    "\n",
    "        # 3. 加入正規化損失 (Regularization Loss)：防止權重過大導致過擬合 (Overfitting)\n",
    "        loss += nn.reg_loss(reg)\n",
    "\n",
    "        # 4. 後向傳播 (Backward Pass)：將梯度傳回網路各層，計算權重與偏置的梯度\n",
    "        nn.backward(loss_grad, reg)\n",
    "\n",
    "        # 5. 更新參數 (Parameter Update)：根據算出的梯度與學習率，調整網路參數\n",
    "        nn.update_parameters(learning_rate)\n",
    "\n",
    "        # 6. 定期列印 Log：每隔 print_n 次就印出目前的損失值，追蹤訓練進度\n",
    "        if epoch % print_n == 0:\n",
    "            print(\"iteration %d: loss %f\" % (epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RQkaTnFaHa4"
   },
   "source": [
    "## 模型訓練範例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0656L3gaxwB"
   },
   "source": [
    "### 範例漩渦資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1769694987380,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "3RBbeuBRavMy",
    "outputId": "9e4c04be-294b-45fa-ccc8-f4e8562c6b2d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 產生螺旋資料\n",
    "X, y = gen_spiral_dataset(N=100, D=2, K=3)\n",
    "\n",
    "# 2. 建立畫布\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# 3. 畫出散佈圖 (Scatter Plot)\n",
    "# X[:, 0] 是橫軸 (x 座標), X[:, 1] 是縱軸 (y 座標)\n",
    "# c=y 代表根據類別自動上色, s=40 是點的大小, edgecolor='k' 是加上黑色邊框比較好辨識\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral, edgecolor='k')\n",
    "\n",
    "# 4. 加上標題與標籤\n",
    "plt.title(\"Spiral Dataset Visualization\")\n",
    "plt.xlabel(\"Feature 1 (x)\")\n",
    "plt.ylabel(\"Feature 2 (y)\")\n",
    "\n",
    "# 5. 顯示圖表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpxE6Q7na24e"
   },
   "source": [
    "### 實際的訓練腳本程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8096,
     "status": "ok",
     "timestamp": 1769695025889,
     "user": {
      "displayName": "Cho-Hsun Lu",
      "userId": "08025011276150415219"
     },
     "user_tz": -480
    },
    "id": "Vtpx-ye51wR3",
    "outputId": "933f8824-b49d-4e5c-9bdd-5fb242bcfff2"
   },
   "outputs": [],
   "source": [
    "# 設定隨機編號（Seed），確保每次執行結果都一樣，方便除錯（Debug）\n",
    "np.random.seed(89)\n",
    "\n",
    "# 產生螺旋狀的測試資料集 X（特徵）與 y（標籤）\n",
    "X, y = gen_spiral_dataset(N=100, D=2, K=3)\n",
    "\n",
    "# 設定訓練參數\n",
    "epochs = 10000            # 總共要跑多少個循環（Epochs）\n",
    "learning_rate = 1e-0      # 學習率（步長），這裡設定為 1.0\n",
    "reg = 1e-4                # 正規化係數（Regularization Strength），用來防止過擬合（Overfitting）\n",
    "print_n = epochs // 10    # 設定每跑 10% 的進度就印一次 Log\n",
    "\n",
    "# # 建立一個神經網路物件\n",
    "# ann = NeuralNetwork()\n",
    "\n",
    "# # 新增第一層全連接層：輸入 2 維，輸出 100 維，使用 ReLU 當啟動函數\n",
    "# ann.add_layer(Dense(2, 100, 'relu'))\n",
    "\n",
    "# # 新增第二層全連接層：輸入 100 維，輸出 3 維，使用 softmax 做分類\n",
    "# ann.add_layer(Dense(100, 3))\n",
    "\n",
    "# 開始訓練模型\n",
    "# 傳入模型物件 nn、資料、損失與梯度的整合函式，以及相關超參數\n",
    "train(nn, X, y, cross_entropy_grad_loss, epochs, learning_rate, reg, print_n)\n",
    "\n",
    "# 訓練完成後，計算模型在訓練集上的準確度（Accuracy）\n",
    "# nn.predict(X) 會回傳預測類別，並與真實標籤 y 比較取平均值\n",
    "print(np.mean(nn.predict(X) == y))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAjBzu2j8hpMZQ5696ejR1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
