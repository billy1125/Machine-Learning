{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b7a1f5",
   "metadata": {},
   "source": [
    "# 卷積神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ca2c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2383221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參數定義：\n",
    "# dz：從下一層傳回的輸出梯度，形狀為 (o_n,)，其中 o_n 是輸出長度\n",
    "# x：輸入數據，形狀為 (n,)，其中 n 是輸入長度\n",
    "# w：卷積核/濾波器，形狀為 (K,)，其中 K 是卷積核長度\n",
    "# pad：Padding 大小，默認為 0\n",
    "# s：步幅，默認為 1\n",
    "def conv_backward(dz, x, w, pad=0, s=1):\n",
    "    # n：輸入長度，K：濾波器/卷積核（Filter）長度\n",
    "    n, K = len(x), len(w)\n",
    "\n",
    "    # o_n (輸出長度)：控制迴圈次數，確保視窗掃描範圍正確\n",
    "    o_n = 1 + (n + 2 * pad - K) // s\n",
    "\n",
    "    assert(o_n == len(dz))    \n",
    "    \n",
    "    # dx (輸入梯度)：準備傳回前一層的梯度；dw (權重梯度)：濾波器/卷積核更新的依據\n",
    "    dx = np.zeros_like(x)\n",
    "    dw = np.zeros_like(w)\n",
    "\n",
    "    # db (Bias 梯度)：輸出梯度 dz 的加總，dz 中每個元素對應輸出的一個位置，對應到每個卷積核的偏置更新\n",
    "    db = dz[:].sum()\n",
    "    \n",
    "    # 進行 Padding 處理與初始化對應梯度空間\n",
    "    x_pad = np.pad(x, [(pad, pad)], 'constant')\n",
    "    dx_pad = np.zeros_like(x_pad)\n",
    "        \n",
    "    for i in range(o_n):\n",
    "        start = i * s\n",
    "\n",
    "        # dw (權重梯度)：由「輸入視窗」與「dz」的乘積累加求得\n",
    "        dw += x_pad[start:start+K] * dz[i]\n",
    "\n",
    "        # dx_pad (輸入梯度)：將權重與 dz 相乘，回傳至對應的輸入位置\n",
    "        dx_pad[start:start+K] += w * dz[i]\n",
    "\n",
    "    # 移除 Padding 取回原始輸入大小的 dx\n",
    "    dx = dx_pad[pad:-pad] if pad > 0 else dx_pad\n",
    "\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "074419bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upstream Gradient (dz):\n",
      "[-1.4255293  -0.3763567  -0.34227539  0.29490764 -0.83732373]\n",
      "\n",
      "Input Gradient (dx):\n",
      "[ 0.50522405 -2.33230266 -0.87796042 -0.03246064  0.67446745]\n",
      "\n",
      "Weight Gradient (dw):\n",
      "[-0.56864738 -0.65679696 -1.09889311]\n",
      "\n",
      "Bias Gradient (db):\n",
      "-2.6865774833459612\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 固定亂數種子，確保實驗結果具備可重複性（Reproducibility）\n",
    "np.random.seed(231)\n",
    "\n",
    "# x (輸入資料)：長度為 5 的向量\n",
    "x = np.random.randn(5)\n",
    "\n",
    "# w (濾波器)：長度為 3 的權重向量\n",
    "w = np.random.randn(3)\n",
    "\n",
    "# s (步幅) 與 pad (填充)：定義卷積滑動的規律與邊界擴充\n",
    "stride = 2\n",
    "pad = 1\n",
    "\n",
    "# dz (下層梯度)：反向傳播的起點，代表「Loss 對卷積輸出的變化率」\n",
    "# 這裡長度設為 5，是為了配合 pad=1, s=1 的卷積輸出長度\n",
    "dz = np.random.randn(5)\n",
    "\n",
    "print(\"Upstream Gradient (dz):\")\n",
    "print(dz)\n",
    "\n",
    "# 執行反向傳播計算\n",
    "# 這裡傳入 s=1，代表以步幅 1 進行梯度回傳計算\n",
    "dx, dw, db = conv_backward(dz, x, w, pad, 1)\n",
    "\n",
    "# dx (輸入梯度)：用來更新前一層神經元（或是上一層卷積層）的數值\n",
    "print(\"\\nInput Gradient (dx):\")\n",
    "print(dx)\n",
    "\n",
    "# dw (權重梯度)：模型訓練最核心的部分，優化器（Optimizer）會根據它來調整 w\n",
    "print(\"\\nWeight Gradient (dw):\")\n",
    "print(dw)\n",
    "\n",
    "# db (Bias 梯度)：Loss 對偏差項的變化率\n",
    "print(\"\\nBias Gradient (db):\")\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b9e2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from modules import init_weights as init_weights\n",
    "\n",
    "# Layer 的基底類別，其他層都會繼承\n",
    "# 定義了基本的接口（forward、backward、正則化相關方法），具體實作由子類別完成\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.params = None  # 存放該層的參數\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward 尚未實作\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, x, grad):\n",
    "        # backward 尚未實作\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reg_grad(self, reg):\n",
    "        # 正則化梯度（預設不做）\n",
    "        pass\n",
    "\n",
    "    def reg_loss(self, reg):\n",
    "        # 正則化 loss（預設為 0）\n",
    "        return 0.\n",
    "\n",
    "    def reg_loss_grad(self, reg):\n",
    "        # 正則化 loss 與梯度（預設為 0）\n",
    "        return 0\n",
    "\n",
    "\n",
    "# 卷積層：實作卷積層的 forward 與 backward，並包含權重初始化與正則化功能\n",
    "# 參數定義：\n",
    "# in_channels：輸入的 channel 數量（例如 RGB 圖片為 3）\n",
    "# out_channels：輸出的 channel 數量（即 filter 的數量）\n",
    "# kernel_size：卷積核的大小（假設為正方形，則為邊長）\n",
    "# stride：卷積的步幅，控制卷積核每次移動的距離\n",
    "# padding：卷積前對輸入進行的零填充大小\n",
    "class Conv(Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "            super().__init__()\n",
    "            self.C = in_channels   # 輸入 channel 數\n",
    "            self.F = out_channels  # filter 數\n",
    "            self.K = kernel_size   # kernel 大小\n",
    "            self.S = stride        # stride\n",
    "            self.P = padding       # padding\n",
    "\n",
    "            # 卷積權重 (F, C, K, K)\n",
    "            self.W = np.random.randn(self.F, self.C, self.K, self.K)\n",
    "\n",
    "            # 每個 filter 一個 bias\n",
    "            self.b = np.random.randn(self.F,)\n",
    "\n",
    "            # 參數與梯度列表\n",
    "            self.params = [self.W, self.b]\n",
    "            self.grads = [np.zeros_like(self.W), np.zeros_like(self.b)]\n",
    "\n",
    "            self.X = None  # forward 時記錄輸入\n",
    "            self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # 使用 Kaiming 初始化權重\n",
    "        init_weights.kaiming_uniform(self.W, a=math.sqrt(5))\n",
    "\n",
    "        # 初始化 bias\n",
    "        if self.b is not None:\n",
    "            fan_in = self.C\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            self.b[:] = np.random.uniform(-bound, bound, self.b.shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 記錄輸入資料\n",
    "        self.X = X\n",
    "\n",
    "        # 取得輸入與權重大小\n",
    "        N, C, X_h, X_w = self.X.shape\n",
    "        F, _, F_h, F_w = self.W.shape\n",
    "\n",
    "        # 對輸入做 padding\n",
    "        X_pad = np.pad(\n",
    "            self.X,\n",
    "            ((0, 0), (0, 0), (self.P, self.P), (self.P, self.P)),\n",
    "            mode='constant',\n",
    "            constant_values=0\n",
    "        )\n",
    "\n",
    "        # 計算輸出大小\n",
    "        O_h = 1 + int((X_h + 2 * self.P - F_h) / self.S)\n",
    "        O_w = 1 + int((X_w + 2 * self.P - F_w) / self.S)\n",
    "\n",
    "        # 建立輸出張量\n",
    "        O = np.zeros((N, F, O_h, O_w))\n",
    "\n",
    "        # 逐位置做卷積運算\n",
    "        for n in range(N): # 遍歷每個輸入樣本\n",
    "            for f in range(F): # 遍歷每個 filter\n",
    "                for i in range(O_h): # 遍歷輸出高度位置\n",
    "                    hs = i * self.S # 計算對應的輸入高度位置\n",
    "                    for j in range(O_w): # 遍歷輸出寬度位置\n",
    "                        ws = j * self.S # 計算對應的輸入寬度位置\n",
    "                        # 進行卷積運算：將輸入視窗與 filter 權重相乘並加上 bias，得到輸出位置的值\n",
    "                        O[n, f, i, j] = (X_pad[n, :, hs:hs+F_h, ws:ws+F_w] * self.W[f]).sum() + self.b[f]\n",
    "\n",
    "        return O\n",
    "\n",
    "    def __call__(self, X):\n",
    "        # 讓 Conv 物件可以直接呼叫\n",
    "        return self.forward(X)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        # 取得輸出梯度與輸入大小\n",
    "        N, F, Z_h, Z_w = dZ.shape\n",
    "        N, C, X_h, X_w = self.X.shape\n",
    "        F, _, F_h, F_w = self.W.shape\n",
    "\n",
    "        pad = self.P\n",
    "\n",
    "        # 計算輸出空間大小\n",
    "        H_ = 1 + (X_h + 2 * pad - F_h) // self.S\n",
    "        W_ = 1 + (X_w + 2 * pad - F_w) // self.S\n",
    "\n",
    "        # 初始化梯度\n",
    "        dX = np.zeros_like(self.X)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "\n",
    "        # padded 的輸入與梯度\n",
    "        X_pad = np.pad(self.X, [(0,0), (0,0), (pad,pad), (pad,pad)], 'constant')\n",
    "        dX_pad = np.pad(dX, [(0,0), (0,0), (pad,pad), (pad,pad)], 'constant')\n",
    "\n",
    "        # 逐位置回傳梯度\n",
    "        for n in range(N): # 遍歷每個輸入樣本\n",
    "            for f in range(F): # 遍歷每個 filter\n",
    "                db[f] += dZ[n, f].sum() # bias 梯度為對應 filter 的輸出梯度總和\n",
    "                for i in range(H_): # 遍歷輸出高度位置\n",
    "                    hs = i * self.S # 計算對應的輸入高度位置\n",
    "                    for j in range(W_): # 遍歷輸出寬度位置\n",
    "                        ws = j * self.S # 計算對應的輸入寬度位置\n",
    "                        # 進行卷積運算的反向傳播：更新權重梯度與輸入梯度\n",
    "                        dW[f] += X_pad[n, :, hs:hs+F_h, ws:ws+F_w] * dZ[n, f, i, j]\n",
    "                        # 輸入梯度由權重與輸出梯度的乘積累加求得，回傳至對應的輸入位置\n",
    "                        dX_pad[n, :, hs:hs+F_h, ws:ws+F_w] += self.W[f] * dZ[n, f, i, j]\n",
    "\n",
    "        # 移除 padding\n",
    "        dX = dX_pad[:, :, pad:pad+X_h, pad:pad+X_h]\n",
    "\n",
    "        # 累加梯度\n",
    "        self.grads[0] += dW\n",
    "        self.grads[1] += db\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def reg_grad(self, reg):\n",
    "        # L2 正則化梯度\n",
    "        self.grads[0] += 2 * reg * self.W\n",
    "\n",
    "    def reg_loss(self, reg):\n",
    "        # L2 正則化 loss\n",
    "        return reg * np.sum(self.W**2)\n",
    "\n",
    "    def reg_loss_grad(self, reg):\n",
    "        # 同時計算正則化 loss 與梯度\n",
    "        self.grads[0] += 2 * reg * self.W\n",
    "        return reg * np.sum(self.W**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef8ecc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Height (O_h): 5\n",
      "Output Width (O_w): 5\n"
     ]
    }
   ],
   "source": [
    "X_h = 5\n",
    "X_w = 5\n",
    "P = 1\n",
    "F_h = 3\n",
    "F_w = 3\n",
    "S = 1\n",
    "O_h = 1 + int((X_h + 2 * P - F_h) / S)\n",
    "O_w = 1 + int((X_w + 2 * P - F_w) / S)\n",
    "print(\"Output Height (O_h):\", O_h)\n",
    "print(\"Output Width (O_w):\", O_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46afcb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3, 5, 5)\n",
      "[[ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763]\n",
      " [-2.3015387   1.74481176 -0.7612069   0.3190391  -0.24937038]\n",
      " [ 1.46210794 -2.06014071 -0.3224172  -0.38405435  1.13376944]\n",
      " [-1.09989127 -0.17242821 -0.87785842  0.04221375  0.58281521]\n",
      " [-1.10061918  1.14472371  0.90159072  0.50249434  0.90085595]] \n",
      "\n",
      "(4, 2, 5, 5)\n",
      "[[ 0.46362714 -0.83578144  0.40298519 -0.32152652  0.56616046]\n",
      " [-0.47878018  1.02346756  0.20004975  0.59663092  0.25253169]\n",
      " [-0.39733747 -0.08368194  0.52454712  0.54133918 -0.32698456]\n",
      " [ 0.47703053 -0.01967369  1.13655418  0.22321357  0.77693417]\n",
      " [-0.23944267  0.62971182 -0.38411731  0.42818679 -0.07566246]] \n",
      "\n",
      "[[-1.30653407  0.07638048  0.36723181  1.23289919 -0.42285696]\n",
      " [ 0.08646441 -2.14246673 -0.83016886  0.45161595  1.10417433]\n",
      " [-0.28173627  2.05635552  1.76024923 -0.06065249 -2.413503  ]\n",
      " [-1.77756638 -0.77785883  1.11584111  0.31027229 -2.09424782]\n",
      " [-0.22876583  1.61336137 -0.37480469 -0.74996962  2.0546241 ]] \n",
      "\n",
      "[[-1.28063939e-02 -3.66152720e-01  8.60100186e-02 -1.22187599e-01\n",
      "  -9.82733000e-02]\n",
      " [ 1.56875134e-01 -1.50855186e-01 -9.11041554e-04 -3.84484585e-01\n",
      "   7.94984888e-02]\n",
      " [-5.68530426e-01  4.20951048e-01  5.41634150e-01  7.61553975e-01\n",
      "  -5.97223756e-01]\n",
      " [ 1.85998058e-01 -3.13055184e-01 -1.49268149e-01 -7.67989087e-01\n",
      "   3.10833619e-01]\n",
      " [ 3.84377541e-02  6.33352468e-01 -3.20074728e-01 -9.61297590e-01\n",
      "   9.84565706e-01]] \n",
      "\n",
      "[[-12.64870544   7.33773197  -3.47470049]\n",
      " [  4.76851832 -18.31687439   3.59104687]\n",
      " [ -3.28925017   0.94823861  -5.66853535]] \n",
      "\n",
      "[11.528173    7.46555585] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 設定隨機數種子，確保每次執行產生的隨機數都一樣，方便除錯與結果復現\n",
    "np.random.seed(1)\n",
    "\n",
    "# x (輸入資料)：維度分別代表 (Batch Size, Channels, Height, Width)\n",
    "# 這裡有 4 筆資料、3 個通道、大小為 5x5\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "\n",
    "print(x.shape)       # 印出輸入維度，確認資料格式是否正確\n",
    "print(x[0,0],\"\\n\")   # 觀察第 1筆資料在第 1 個通道的數值，確認資料內容是否合理\n",
    "\n",
    "# 初始化卷積層：輸入 3 通道、輸出 2 通道、Filter 3x3、步幅(Stride) 1、填充(Padding) 1\n",
    "conv = Conv(3, 2, 3, 1, 1)\n",
    "\n",
    "# f (輸出特徵圖)：執行前向傳播計算結果\n",
    "f = conv.forward(x)\n",
    "\n",
    "print(f.shape)       # 印出輸出維度，確認 Padding 與 Stride 計算是否正確\n",
    "print(f[0,0],\"\\n\")   # 觀察第 1 筆資料在第 1 個輸出通道的特徵數值\n",
    "\n",
    "# df (輸出梯度)：模擬從下一層傳回來的梯度 (Upstream Gradient)\n",
    "# 其形狀必須與前向傳播的輸出 f 完全一致\n",
    "df = np.random.randn(4, 2, 5, 5)\n",
    "\n",
    "# dx (輸入梯度)：執行反向傳播，計算要傳回前一層的梯度\n",
    "dx = conv.backward(df)\n",
    "\n",
    "print(df[0,0],\"\\n\")  # 印出模擬的原始輸出梯度\n",
    "print(dx[0,0],\"\\n\")  # 印出計算後對輸入 x 的梯度\n",
    "\n",
    "# conv.grads (參數梯度)：儲存了此層需要被更新的權重與偏差梯度\n",
    "print(conv.grads[0][0,0],\"\\n\") # 印出濾波器權重的梯度 (dw)\n",
    "print(conv.grads[1],\"\\n\")      # 印出偏差的梯度 (db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a8d1336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.674960501522719e-11\n",
      "1.2954970429746027e-11\n",
      "4.92105245442076e-11\n"
     ]
    }
   ],
   "source": [
    "def numerical_gradient_from_df(f, p, df, h=1e-5):\n",
    "  # 建立一個與 p 形狀相同、內容全為 0 的陣列，用來存每個參數的梯度\n",
    "  grad = np.zeros_like(p)\n",
    "\n",
    "  # 使用 nditer 逐一走訪 p 中的每一個元素（支援多維陣列）\n",
    "  it = np.nditer(p, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "  # 只要 iterator 還沒跑完就持續計算\n",
    "  while not it.finished:\n",
    "    # 取得目前走訪到的索引位置\n",
    "    idx = it.multi_index\n",
    "\n",
    "    # 先把原本的參數值存起來\n",
    "    oldval = p[idx]\n",
    "\n",
    "    # 將該參數往正方向微調一點\n",
    "    p[idx] = oldval + h\n",
    "    pos = f()       # 在參數被改動後重新呼叫 f()，取得正方向的輸出結果\n",
    "\n",
    "    # 將該參數往負方向微調一點\n",
    "    p[idx] = oldval - h\n",
    "    neg = f()       # 在參數被改動後重新呼叫 f()，取得負方向的輸出結果\n",
    "\n",
    "    # 將參數值還原成原本的狀態，避免影響下一次計算\n",
    "    p[idx] = oldval\n",
    "\n",
    "    # 使用中央差分法計算梯度，並與上游傳下來的 df 做加權\n",
    "    grad[idx] = np.sum((pos - neg) * df) / (2 * h)\n",
    "\n",
    "    # 另一種寫法（使用內積），目前被註解掉\n",
    "    # grad[idx] = np.dot((pos - neg), df) / (2 * h)\n",
    "\n",
    "    # 移動到下一個參數位置\n",
    "    it.iternext()\n",
    "\n",
    "  # 回傳整個參數 p 的數值梯度\n",
    "  return grad\n",
    "\n",
    "def f():\n",
    "   return conv.forward(x)\n",
    "\n",
    "dw_num = numerical_gradient_from_df(f,conv.W,df)\n",
    "diff_error = lambda x, y: np.max(np.abs(x - y)) \n",
    "print(diff_error(conv.grads[0],dw_num))\n",
    "\n",
    "db_num = numerical_gradient_from_df(lambda :conv.forward(x),conv.b,df)\n",
    "print(diff_error(conv.grads[1],db_num))\n",
    "\n",
    "dx_num = numerical_gradient_from_df(lambda :conv.forward(x),x,df)\n",
    "print(diff_error(dx,dx_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5750c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pool(Layer):\n",
    "    def __init__(self, pool_param = (2,2,2)):\n",
    "        super().__init__()\n",
    "        # pool_h, pool_w: 池化視窗的長與寬；stride: 步幅\n",
    "        self.pool_h, self.pool_w, self.stride = pool_param\n",
    "\n",
    "    def forward(self, x): \n",
    "        self.x = x    \n",
    "        N, C, H, W = x.shape\n",
    "        pool_h, pool_w, stride = self.pool_h, self.pool_w, self.stride\n",
    "        \n",
    "        # h_out, w_out (輸出維度)：計算公式與卷積層相似，決定輸出特徵圖的大小\n",
    "        h_out = 1 + (H - pool_h) // stride\n",
    "        w_out = 1 + (W - pool_w) // stride         \n",
    "        out = np.zeros((N, C, h_out, w_out))\n",
    "        \n",
    "        # 透過四層迴圈遍歷 Batch(N)、通道(C) 與 空間位置(i, j)\n",
    "        for n in range(N): # 遍歷每個輸入樣本\n",
    "            for c in range(C):  # 遍歷每個通道\n",
    "                for i in range(h_out): # 遍歷輸出高度位置\n",
    "                    si = stride * i  # 計算對應的輸入高度位置\n",
    "                    for j in range(w_out): # 遍歷輸出寬度位置\n",
    "                        sj = stride * j # 計算對應的輸入寬度位置\n",
    "                        # x_win (輸入視窗)：擷取目前要進行池化的局部區域\n",
    "                        x_win = x[n, c, si:si+pool_h, sj:sj+pool_w]  \n",
    "                        # Max Pooling：只取出視窗中的最大值作為輸出\n",
    "                        out[n,c,i,j] = np.max(x_win)        \n",
    "     \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, C, H, W = x.shape\n",
    "        kH, kW, stride = self.pool_h, self.pool_w, self.stride      \n",
    "        oH = 1 + (H - kH) // stride\n",
    "        oW = 1 + (W - kW) // stride\n",
    "       \n",
    "        # dx (輸入梯度)：初始化為 0，形狀與原始輸入 x 相同\n",
    "        dx = np.zeros_like(x)    \n",
    "  \n",
    "        # dout (下層梯度)：從後方傳回對此層輸出的梯度，是計算的反向起點\n",
    "        for k in range(N): # 遍歷每個輸入樣本\n",
    "            for l in range(C): # 遍歷每個通道\n",
    "                for i in range(oH): # 遍歷輸出高度位置\n",
    "                    si = stride * i # 計算對應的輸入高度位置\n",
    "                    for j in range(oW): # 遍歷輸出寬度位置\n",
    "                        sj = stride * j # 計算對應的輸入寬度位置\n",
    "                        # slice: 重新找出前向傳播時的視窗區域\n",
    "                        slice = x[k, l, si:si+kH, sj:sj+kW]\n",
    "                        slice_max = np.max(slice)\n",
    "                        \n",
    "                        # 梯度回傳核心：利用布林遮罩 (slice_max == slice)\n",
    "                        # 只有最大值的位置會接收來自 dout 的梯度，其餘維持 0\n",
    "                        dx[k, l, si:si+kH, sj:sj+kW] += (slice_max == slice) * dout[k,l,i,j] \n",
    "                    \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f58443d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.680655614677562e-11\n"
     ]
    }
   ],
   "source": [
    "# 初始化輸入資料 x：(Batch, Channel, Height, Width) = (3, 2, 8, 8)\n",
    "x = np.random.randn(3, 2, 8, 8)\n",
    "\n",
    "# df (輸出端梯度)：模擬從下一層傳回來的梯度\n",
    "# 因為 8x8 經過 2x2 且 Stride 為 2 的池化後，輸出會變成 4x4\n",
    "df = np.random.randn(3, 2, 4, 4)\n",
    "\n",
    "# 定義池化層：視窗大小 (2,2)，步幅 (Stride) 為 2\n",
    "pool = Pool((2, 2, 2))\n",
    "\n",
    "# f (前向傳播輸出)：執行池化運算\n",
    "f = pool.forward(x)\n",
    "\n",
    "# dx (解析梯度)：執行我們在 Pool 類別中寫的 backward 邏輯\n",
    "dx = pool.backward(df)\n",
    "\n",
    "# dx_num (數值梯度)：利用微小擾動（如 +0.0001 與 -0.0001）計算出的梯度\n",
    "# 這是利用數學定義跑出來的「標準答案」，用來驗證 dx 是否寫錯\n",
    "dx_num = numerical_gradient_from_df(lambda :pool.forward(x), x, df)\n",
    "\n",
    "# 印出誤差比對：如果數值非常小（如 1e-10），代表你的 backward 邏輯寫對了！\n",
    "print(diff_error(dx, dx_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
