{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b7a1f5",
   "metadata": {},
   "source": [
    "# 卷積神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ca2c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2383221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dz, x, w, pad=0, s=1):\n",
    "    # n：輸入資料長度，K：filter（權重）長度\n",
    "    n, K = len(x), len(w)\n",
    "\n",
    "    # 計算輸出的長度\n",
    "    o_n = 1 + (n + 2 * pad - K) // s\n",
    "\n",
    "    # 確認輸出長度與 dz 長度一致\n",
    "    assert(o_n == len(dz))    \n",
    "    \n",
    "    # 初始化 x 的梯度\n",
    "    dx = np.zeros_like(x)\n",
    "\n",
    "    # 初始化權重的梯度\n",
    "    dw = np.zeros_like(w)\n",
    "\n",
    "    # bias 的梯度，直接把 dz 全部加起來\n",
    "    db = dz[:].sum()\n",
    "    \n",
    "    # 對輸入資料做 padding\n",
    "    x_pad = np.pad(x, [(pad, pad)], 'constant')\n",
    "\n",
    "    # padded 輸入對應的梯度\n",
    "    dx_pad = np.zeros_like(x_pad)\n",
    "        \n",
    "    # 逐步計算每個位置的反向傳播\n",
    "    for i in range(o_n):\n",
    "        # 計算目前視窗的起始位置\n",
    "        start = i * s\n",
    "\n",
    "        # 計算權重的梯度\n",
    "        dw += x_pad[start:start+K] * dz[i]\n",
    "\n",
    "        # 將梯度回傳到對應的輸入位置\n",
    "        dx_pad[start:start+K] += w * dz[i]\n",
    "\n",
    "    # 移除 padding，取得原始輸入大小的梯度\n",
    "    dx = dx_pad[pad:-pad]    \n",
    "\n",
    "    # 回傳輸入梯度、權重梯度與 bias 梯度\n",
    "    return dx, dw, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "074419bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.4255293  -0.3763567  -0.34227539  0.29490764 -0.83732373]\n",
      "[ 0.50522405 -2.33230266 -0.87796042 -0.03246064  0.67446745]\n",
      "[-0.56864738 -0.65679696 -1.09889311]\n",
      "-2.6865774833459612\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 固定亂數種子，確保每次結果都一樣，方便除錯與驗證\n",
    "np.random.seed(231)\n",
    "\n",
    "# 建立長度為 5 的輸入資料（1D）\n",
    "x = np.random.randn(5)\n",
    "\n",
    "# 建立長度為 3 的權重（filter）\n",
    "w = np.random.randn(3)\n",
    "\n",
    "# stride 設為 2（此處實際呼叫時未使用這個變數）\n",
    "stride = 2\n",
    "\n",
    "# padding 設為 1\n",
    "pad = 1\n",
    "\n",
    "# 建立輸出梯度 dz，長度為 5\n",
    "dz = np.random.randn(5)\n",
    "\n",
    "# 印出 dz，確認反向傳播的輸入梯度\n",
    "print(dz)\n",
    "\n",
    "# 呼叫 convolution 的 backward 函式\n",
    "# 傳入 dz、輸入 x、權重 w、padding 與 stride\n",
    "dx, dw, db = conv_backward(dz, x, w, pad, 1)\n",
    "\n",
    "# 印出對輸入 x 的梯度\n",
    "print(dx)\n",
    "\n",
    "# 印出對權重 w 的梯度\n",
    "print(dw)\n",
    "\n",
    "# 印出對 bias 的梯度\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b9e2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from init_weights import *\n",
    "\n",
    "# Layer 的基底類別，其他層都會繼承\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.params = None  # 存放該層的參數\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward 尚未實作\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, x, grad):\n",
    "        # backward 尚未實作\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reg_grad(self, reg):\n",
    "        # 正則化梯度（預設不做）\n",
    "        pass\n",
    "\n",
    "    def reg_loss(self, reg):\n",
    "        # 正則化 loss（預設為 0）\n",
    "        return 0.\n",
    "\n",
    "    def reg_loss_grad(self, reg):\n",
    "        # 正則化 loss 與梯度（預設為 0）\n",
    "        return 0\n",
    "\n",
    "\n",
    "# 卷積層\n",
    "class Conv(Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "            super().__init__()\n",
    "            self.C = in_channels   # 輸入 channel 數\n",
    "            self.F = out_channels  # filter 數\n",
    "            self.K = kernel_size   # kernel 大小\n",
    "            self.S = stride        # stride\n",
    "            self.P = padding       # padding\n",
    "\n",
    "            # 卷積權重 (F, C, K, K)\n",
    "            self.W = np.random.randn(self.F, self.C, self.K, self.K)\n",
    "\n",
    "            # 每個 filter 一個 bias\n",
    "            self.b = np.random.randn(self.F,)\n",
    "\n",
    "            # 參數與梯度列表\n",
    "            self.params = [self.W, self.b]\n",
    "            self.grads = [np.zeros_like(self.W), np.zeros_like(self.b)]\n",
    "\n",
    "            self.X = None  # forward 時記錄輸入\n",
    "            self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # 使用 Kaiming 初始化權重\n",
    "        kaiming_uniform(self.W, a=math.sqrt(5))\n",
    "\n",
    "        # 初始化 bias\n",
    "        if self.b is not None:\n",
    "            fan_in = self.C\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            self.b[:] = np.random.uniform(-bound, bound, self.b.shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 記錄輸入資料\n",
    "        self.X = X\n",
    "\n",
    "        # 取得輸入與權重大小\n",
    "        N, C, X_h, X_w = self.X.shape\n",
    "        F, _, F_h, F_w = self.W.shape\n",
    "\n",
    "        # 對輸入做 padding\n",
    "        X_pad = np.pad(\n",
    "            self.X,\n",
    "            ((0, 0), (0, 0), (self.P, self.P), (self.P, self.P)),\n",
    "            mode='constant',\n",
    "            constant_values=0\n",
    "        )\n",
    "\n",
    "        # 計算輸出大小\n",
    "        O_h = 1 + int((X_h + 2 * self.P - F_h) / self.S)\n",
    "        O_w = 1 + int((X_w + 2 * self.P - F_w) / self.S)\n",
    "\n",
    "        # 建立輸出張量\n",
    "        O = np.zeros((N, F, O_h, O_w))\n",
    "\n",
    "        # 逐位置做卷積運算\n",
    "        for n in range(N):\n",
    "            for f in range(F):\n",
    "                for i in range(O_h):\n",
    "                    hs = i * self.S\n",
    "                    for j in range(O_w):\n",
    "                        ws = j * self.S\n",
    "                        O[n, f, i, j] = (\n",
    "                            X_pad[n, :, hs:hs+F_h, ws:ws+F_w] * self.W[f]\n",
    "                        ).sum() + self.b[f]\n",
    "\n",
    "        return O\n",
    "\n",
    "    def __call__(self, X):\n",
    "        # 讓 Conv 物件可以直接呼叫\n",
    "        return self.forward(X)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        # 取得輸出梯度與輸入大小\n",
    "        N, F, Z_h, Z_w = dZ.shape\n",
    "        N, C, X_h, X_w = self.X.shape\n",
    "        F, _, F_h, F_w = self.W.shape\n",
    "\n",
    "        pad = self.P\n",
    "\n",
    "        # 計算輸出空間大小\n",
    "        H_ = 1 + (X_h + 2 * pad - F_h) // self.S\n",
    "        W_ = 1 + (X_w + 2 * pad - F_w) // self.S\n",
    "\n",
    "        # 初始化梯度\n",
    "        dX = np.zeros_like(self.X)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "\n",
    "        # padded 的輸入與梯度\n",
    "        X_pad = np.pad(self.X, [(0,0), (0,0), (pad,pad), (pad,pad)], 'constant')\n",
    "        dX_pad = np.pad(dX, [(0,0), (0,0), (pad,pad), (pad,pad)], 'constant')\n",
    "\n",
    "        # 逐位置回傳梯度\n",
    "        for n in range(N):\n",
    "            for f in range(F):\n",
    "                db[f] += dZ[n, f].sum()\n",
    "                for i in range(H_):\n",
    "                    hs = i * self.S\n",
    "                    for j in range(W_):\n",
    "                        ws = j * self.S\n",
    "                        dW[f] += X_pad[n, :, hs:hs+F_h, ws:ws+F_w] * dZ[n, f, i, j]\n",
    "                        dX_pad[n, :, hs:hs+F_h, ws:ws+F_w] += self.W[f] * dZ[n, f, i, j]\n",
    "\n",
    "        # 移除 padding\n",
    "        dX = dX_pad[:, :, pad:pad+X_h, pad:pad+X_h]\n",
    "\n",
    "        # 累加梯度\n",
    "        self.grads[0] += dW\n",
    "        self.grads[1] += db\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def reg_grad(self, reg):\n",
    "        # L2 正則化梯度\n",
    "        self.grads[0] += 2 * reg * self.W\n",
    "\n",
    "    def reg_loss(self, reg):\n",
    "        # L2 正則化 loss\n",
    "        return reg * np.sum(self.W**2)\n",
    "\n",
    "    def reg_loss_grad(self, reg):\n",
    "        # 同時計算正則化 loss 與梯度\n",
    "        self.grads[0] += 2 * reg * self.W\n",
    "        return reg * np.sum(self.W**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46afcb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2, 5, 5)\n",
      "[[ 0.46362714 -0.83578144  0.40298519 -0.32152652  0.56616046]\n",
      " [-0.47878018  1.02346756  0.20004975  0.59663092  0.25253169]\n",
      " [-0.39733747 -0.08368194  0.52454712  0.54133918 -0.32698456]\n",
      " [ 0.47703053 -0.01967369  1.13655418  0.22321357  0.77693417]\n",
      " [-0.23944267  0.62971182 -0.38411731  0.42818679 -0.07566246]] \n",
      "\n",
      "[[-1.30653407  0.07638048  0.36723181  1.23289919 -0.42285696]\n",
      " [ 0.08646441 -2.14246673 -0.83016886  0.45161595  1.10417433]\n",
      " [-0.28173627  2.05635552  1.76024923 -0.06065249 -2.413503  ]\n",
      " [-1.77756638 -0.77785883  1.11584111  0.31027229 -2.09424782]\n",
      " [-0.22876583  1.61336137 -0.37480469 -0.74996962  2.0546241 ]] \n",
      "\n",
      "[[-1.28063939e-02 -3.66152720e-01  8.60100186e-02 -1.22187599e-01\n",
      "  -9.82733000e-02]\n",
      " [ 1.56875134e-01 -1.50855186e-01 -9.11041554e-04 -3.84484585e-01\n",
      "   7.94984888e-02]\n",
      " [-5.68530426e-01  4.20951048e-01  5.41634150e-01  7.61553975e-01\n",
      "  -5.97223756e-01]\n",
      " [ 1.85998058e-01 -3.13055184e-01 -1.49268149e-01 -7.67989087e-01\n",
      "   3.10833619e-01]\n",
      " [ 3.84377541e-02  6.33352468e-01 -3.20074728e-01 -9.61297590e-01\n",
      "   9.84565706e-01]] \n",
      "\n",
      "[[-12.64870544   7.33773197  -3.47470049]\n",
      " [  4.76851832 -18.31687439   3.59104687]\n",
      " [ -3.28925017   0.94823861  -5.66853535]] \n",
      "\n",
      "[11.528173    7.46555585] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "\n",
    "conv = Conv(3,2,3,1,1)\n",
    "f = conv.forward(x)\n",
    "print(f.shape)\n",
    "print(f[0,0],\"\\n\")\n",
    "\n",
    "df = np.random.randn(4, 2, 5, 5)\n",
    "dx= conv.backward(df)\n",
    "print(df[0,0],\"\\n\")\n",
    "print(dx[0,0],\"\\n\")\n",
    "print(conv.grads[0][0,0],\"\\n\")\n",
    "print(conv.grads[1],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a8d1336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.674960501522719e-11\n",
      "1.2954970429746027e-11\n",
      "4.92105245442076e-11\n"
     ]
    }
   ],
   "source": [
    "def numerical_gradient_from_df(f, p, df, h=1e-5):\n",
    "  # 建立一個與 p 形狀相同、內容全為 0 的陣列，用來存每個參數的梯度\n",
    "  grad = np.zeros_like(p)\n",
    "\n",
    "  # 使用 nditer 逐一走訪 p 中的每一個元素（支援多維陣列）\n",
    "  it = np.nditer(p, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "  # 只要 iterator 還沒跑完就持續計算\n",
    "  while not it.finished:\n",
    "    # 取得目前走訪到的索引位置\n",
    "    idx = it.multi_index\n",
    "\n",
    "    # 先把原本的參數值存起來\n",
    "    oldval = p[idx]\n",
    "\n",
    "    # 將該參數往正方向微調一點\n",
    "    p[idx] = oldval + h\n",
    "    pos = f()       # 在參數被改動後重新呼叫 f()，取得正方向的輸出結果\n",
    "\n",
    "    # 將該參數往負方向微調一點\n",
    "    p[idx] = oldval - h\n",
    "    neg = f()       # 在參數被改動後重新呼叫 f()，取得負方向的輸出結果\n",
    "\n",
    "    # 將參數值還原成原本的狀態，避免影響下一次計算\n",
    "    p[idx] = oldval\n",
    "\n",
    "    # 使用中央差分法計算梯度，並與上游傳下來的 df 做加權\n",
    "    grad[idx] = np.sum((pos - neg) * df) / (2 * h)\n",
    "\n",
    "    # 另一種寫法（使用內積），目前被註解掉\n",
    "    # grad[idx] = np.dot((pos - neg), df) / (2 * h)\n",
    "\n",
    "    # 移動到下一個參數位置\n",
    "    it.iternext()\n",
    "\n",
    "  # 回傳整個參數 p 的數值梯度\n",
    "  return grad\n",
    "\n",
    "def f():\n",
    "   return conv.forward(x)\n",
    "\n",
    "dw_num = numerical_gradient_from_df(f,conv.W,df)\n",
    "diff_error = lambda x, y: np.max(np.abs(x - y)) \n",
    "print(diff_error(conv.grads[0],dw_num))\n",
    "\n",
    "db_num = numerical_gradient_from_df(lambda :conv.forward(x),conv.b,df)\n",
    "print(diff_error(conv.grads[1],db_num))\n",
    "\n",
    "dx_num = numerical_gradient_from_df(lambda :conv.forward(x),x,df)\n",
    "print(diff_error(dx,dx_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
